{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "深度学习包括模型、优化器、损失函数<br/>\n",
    "实现以下函数：\n",
    "- _create_slots         定义内部变量\n",
    "- _prepare_local        更新内部参数\n",
    "- _resource_apply_dense 反向传播，计算梯度\n",
    "\n",
    "目标：实现Adam算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, learning_rate=0.1, beta1=0.9, beta2=0.999, eps=1e-7, name=\"Adam\"):\n",
    "        super(Adam, self).__init__(name)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1         = beta1\n",
    "        self.beta2         = beta2\n",
    "        self.eps           = eps\n",
    "    \n",
    "    def _create_slots(self, var_list):\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, 'm1')\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, 'v1')\n",
    "    \n",
    "    def _prepare_local(self, var_device, var_dtype, apply_state):\n",
    "        super(Adam, self)._prepare_local(var_device, var_dtype, apply_state)\n",
    "\n",
    "        local_step = tf.cast(self.iterations + 1, var_dtype)\n",
    "        # With {\\beta_1^t} and {\\beta_2^t} we denote {\\beta_1} and {\\beta_2} to the power t.\n",
    "        beta1_power = tf.pow(self.beta1, local_step)\n",
    "        beta2_power = tf.pow(self.beta2, local_step)\n",
    "        apply_state[(var_device, var_dtype)].update(\n",
    "            dict(\n",
    "                beta1_power=beta1_power,\n",
    "                beta2_power=beta2_power,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def _resource_apply_dense(self, grad, var, apply_state=None):\n",
    "        var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "        coefficients = apply_state.get((var_device, var_dtype))\n",
    "        beta1_power = coefficients['beta1_power']\n",
    "        beta2_power = coefficients['beta2_power']\n",
    "        v = self.get_slot(var, \"v1\")\n",
    "        v_t = v.assign(self.beta2 * v + (1. - self.beta2) * grad**2)\n",
    "        m = self.get_slot(var, \"m1\")\n",
    "        m_t = m.assign(self.beta1 * m + (1. - self.beta1) * grad)\n",
    "        # Note that the efficiency of algorithm 1 can, at the expense of clarity, \n",
    "        # be improved upon by changing the order of computation\n",
    "        alpha_t =  tf.sqrt(1 - beta2_power) / (1 - beta1_power)\n",
    "        g_t =  (m_t*alpha_t) / (tf.sqrt(v_t) + self.eps)\n",
    "        var_update = state_ops.assign_sub(var, self.learning_rate * g_t)\n",
    "        return tf.group(*[var_update, v_t, m_t])\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var):\n",
    "        raise NotImplementedError(\"Sparse gradient updates are not supported.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
